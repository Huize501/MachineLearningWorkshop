{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow Datasets and Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets and Estimators are two key TensorFlow features:\n",
    "\n",
    "- Datasets: Best practice way of creating input pipelines. Reading data in to your graph. \n",
    "- Estimators: A high-level API to create TensorFlow models. Estimators include canned models (out of the box) and custom estimators.\n",
    "\n",
    "Below you the TensorFlow architecture including the dataset API an Estimators. Combined, they offer an easy way to create TensorFlow models:\n",
    "\n",
    "![title](https://3.bp.blogspot.com/-l2UT45WGdyw/Wbe7au1nfwI/AAAAAAAAD1I/GeQcQUUWezIiaFFRCiMILlX2EYdG49C0wCLcBGAs/s1600/image6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model categorizes Iris flowers based on four botanical features (sepal length, sepal width, petal length, and petal width). So, during inference, you can provide values for those four features and the model will predict that the flower is one of the following three beautiful variants:\n",
    "\n",
    "![title](https://www.tensorflow.org/images/iris_three_species.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import six.moves.urllib.request as request\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Check that we have correct TensorFlow version installed\n",
    "tf_version = tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to train a Deep Neural Network Classifier with the below structure. All input and output values will be float32, and the sum of the output values will be 1 (as we are predicting the probability for each individual Iris type):\n",
    "\n",
    "![title](https://1.bp.blogspot.com/-EEdRK1mK1QQ/Wbe7qPWECZI/AAAAAAAAD1Q/fjnpGIiRIosTZ3YupkgiKJVaBtPg8KvGwCLcBGAs/s1600/image3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get our Data\n",
    "\n",
    "First we need to fetch our data. It can be downloaded as csv from our Tensorflow website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Path to where data will be stored \n",
    "PATH = \"/tmp/tf_dataset_and_estimator_apis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fetch and store Training and Test dataset files\n",
    "PATH_DATASET = PATH + os.sep + \"dataset\"\n",
    "FILE_TRAIN = PATH_DATASET + os.sep + \"iris_training.csv\"\n",
    "FILE_TEST = PATH_DATASET + os.sep + \"iris_test.csv\"\n",
    "URL_TRAIN = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "URL_TEST = \"http://download.tensorflow.org/data/iris_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function will fetch (download) the data.\n",
    "\n",
    "def downloadDataset(url, file):\n",
    "    if not os.path.exists(PATH_DATASET):\n",
    "        os.makedirs(PATH_DATASET)\n",
    "    if not os.path.exists(file):\n",
    "        data = request.urlopen(url).read()\n",
    "        with open(file, \"wb\") as f:\n",
    "            f.write(data)\n",
    "            f.close()\n",
    "downloadDataset(URL_TRAIN, FILE_TRAIN)\n",
    "downloadDataset(URL_TEST, FILE_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Metadata\n",
    "\n",
    "To describe our dataset, we first create a list of our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:**\n",
    "\n",
    "- *Create a list that stores all the input feature names. Tip: Have a look at the image of the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the features. \n",
    "\n",
    "feature_names =  # Put your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train our model, we'll need a function that reads the input file and returns the feature and label data. Estimators requires that you create a function of the following format. The return value must be a two-element tuple organized as follows: :\n",
    "\n",
    "- The first element must be a dict in which each input feature is a key, and then a list of values for the training batch.\n",
    "- The second element is a list of labels for the training batch.\n",
    "\n",
    "Since we are returning a batch of input features and training labels, it means that all lists in the return statement will have equal lengths. Technically speaking, whenever we referred to \"list\" here, we actually mean a 1-d TensorFlow tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
    "    def decode_csv(line):\n",
    "        # Convert CSV records to tensors. Each column maps to one tensor.\n",
    "        parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])\n",
    "        label = parsed_line[-1]  # Last element is the label\n",
    "        del parsed_line[-1]  # Delete last element (label)\n",
    "        features = parsed_line  # Everything but last elements are the features\n",
    "        d = dict(zip(feature_names, features)), label # This will return the tupel\n",
    "        return d\n",
    "    \n",
    "    #A Dataset comprising lines from one or more text files\n",
    "    dataset = (tf.data.TextLineDataset(file_path)  # Read text file\n",
    "               .skip(1)  # Skip header row\n",
    "               .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "    \n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n",
    "    dataset = dataset.batch(32)  # Batch size to use\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note the following: :\n",
    "- TextLineDataset: The Dataset API will do a lot of memory management for you when you're using its file-based datasets. You can, for example, read in dataset files much larger than memory or read in multiple files by specifying a list as argument.\n",
    "- shuffle: Reads buffer_size records, then shuffles (randomizes) their order.\n",
    "- map: Calls the decode_csv function with each element in the dataset as an argument (since we are using TextLineDataset, each element will be a line of CSV text). Then we apply decode_csv to each of the lines.\n",
    "- decode_csv: Splits each line into fields, providing the default values if necessary. Then returns a dict with the field keys and field values. The map function updates each elem (line) in the dataset with the dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_batch = input_fn(FILE_TRAIN, True) # Will return 32 random elements\n",
    "\n",
    "# Now let's try it out, retrieving and printing one batch of data.\n",
    "# Although this code looks strange, you don't need to understand\n",
    "# the details.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    first_batch = sess.run(next_batch)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and specify your Estimator\n",
    "\n",
    "Estimators is a high-level API that reduces much of the boilerplate code you previously needed to write when training a TensorFlow model. Estimators are also very flexible, allowing you to override the default behavior if you have specific requirements for your model.\n",
    "\n",
    "There are two possible ways you can build your model using Estimators:\n",
    "\n",
    "Pre-made Estimator - These are predefined estimators, created to generate a specific type of model. In this blog post, we will use the DNNClassifier pre-made estimator.\n",
    "Estimator (base class) - Gives you complete control of how your model should be created by using a model_fn function. We will cover how to do this in a separate blog post.\n",
    "<p>\n",
    "Here is the class diagram for Estimators:\n",
    "\n",
    "![title](https://1.bp.blogspot.com/-njTtnjOq_cE/Wbe772URrgI/AAAAAAAAD1Y/h1mWj6MGSzYg_KDuVXWBYeNqA4z5WRSpACLcBGAs/s1600/image2.jpg)\n",
    "\n",
    "<p>\n",
    "As you can see, all estimators make use of input_fn that provides the estimator with input data. In our case, we will reuse my_input_fn, which we defined for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the feature_columns, which specifies the input to our model\n",
    "# In this case our input features are numeric, so use numeric_column for each one\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we specify some of the hyperparameters and metadata,\n",
    "\n",
    "num_hidden_units =[512, 256, 128] # Number of neurons and hidden layers\n",
    "directory = './Checkpoints/checkpoints_tutorial17-1/\"' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we instantiate the estimator. In this case we are using a DNN Classifier\n",
    "\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns, \n",
    "    hidden_units=num_hidden_units,\n",
    "    n_classes = 3, \n",
    "    model_dir=directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have everything in place and we can start the training. \n",
    "\n",
    "classifier.train(\n",
    "    input_fn=lambda: input_fn(FILE_TRAIN, True, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait a minute... what is this \"lambda: my_input_fn(FILE_TRAIN, True, 8)\" stuff? That is where we hook up Datasets with the Estimators! Estimators needs data to perform training, evaluation, and prediction, and it uses the input_fn to fetch the data. Estimators require an input_fn with no arguments, so we create a function with no arguments using lambda, which calls our input_fn with the desired arguments: the file_path, shuffle setting, and repeat_count. In our case, we use our my_input_fn, passing it:\n",
    "\n",
    "- FILE_TRAIN, which is the training data file.\n",
    "- True, which tells the Estimator to shuffle the data.\n",
    "- 8, which tells the Estimator to and repeat the dataset 8 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate our model using the examples contained in FILE_TEST\n",
    "# Return value will contain evaluation_metrics such as: loss & average_loss\n",
    "evaluate_result = classifier.evaluate(\n",
    "    input_fn=lambda: input_fn(FILE_TEST, False, 4))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:**\n",
    "\n",
    "- *Change some of the hyperparamaters of the model to see if you can* improve the performance. \n",
    "- *What is the activation function of the DNN Classifier? How can you change it?*\n",
    "- *Tweak and change more of the code and see how things work*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:**\n",
    "\n",
    "- *Test a different model: Maybe Linear Classifier?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your new model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do a prediction\n",
    "\n",
    "And that's it! We now have a trained model, and if we are happy with the evaluation results, we can use it to predict an Iris flower based on some input. As with training, and evaluation, we make predictions using a single function call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let create a dataset for prediction\n",
    "# We've taken the first 3 examples in FILE_TEST\n",
    "prediction_input = [[5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n",
    "                    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris Virginica\n",
    "                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Sentosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_input_fn():\n",
    "    def decode(x):\n",
    "        x = tf.split(x, 4)  # Need to split into our 4 features\n",
    "        return dict(zip(feature_names, x))  # To build a dict of them\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(prediction_input)\n",
    "    dataset = dataset.map(decode)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_feature_batch = iterator.get_next()\n",
    "    return next_feature_batch, None  # In prediction, we have no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict all our prediction_input\n",
    "predict_results = classifier.predict(input_fn=new_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"Predictions:\")\n",
    "for idx, prediction in enumerate(predict_results):\n",
    "    type = prediction[\"class_ids\"][0]  # Get the predicted class (index)\n",
    "    if type == 0:\n",
    "        print(\"  I think: {}, is Iris Sentosa\".format(prediction_input[idx]))\n",
    "    elif type == 1:\n",
    "        print(\"  I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n",
    "    else:\n",
    "        print(\"  I think: {}, is Iris Virginica\".format(prediction_input[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freebies\n",
    "\n",
    "**Exercise 4:**\n",
    "\n",
    "- *Checkout Tensorboard and see if you can get it working*\n",
    "- *Checkout your performance metrics and the graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
